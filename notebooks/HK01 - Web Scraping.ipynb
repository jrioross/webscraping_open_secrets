{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of Political Contributions During the 2020 House of Representatives Election\n",
    "\n",
    "Goal of this notebook is to webscrape data for the 2020 House of Representatives Election from the Open Secrets website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Prepare list of all URLs\n",
    "# Initialize empty list for URLs\n",
    "URLs = []\n",
    "\n",
    "# Scrape table of state abbreviations and # of representatives per state from Britannica site\n",
    "response = requests.get('https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120')\n",
    "soup = BS(response.text)\n",
    "states_reps = pd.read_html(str(soup.find('table')))[0].drop(50)\n",
    "\n",
    "# Scrape abbreviation table from World Population Review site\n",
    "response = requests.get('https://worldpopulationreview.com/states/state-abbreviations')\n",
    "soup = BS(response.text)\n",
    "states_abv = pd.read_html(str(soup.find('table')))[0]\n",
    "states_abv = (states_abv.rename({'State':'state'},\n",
    "                                axis='columns'))\n",
    "\n",
    "# Merge the two read in dataframes\n",
    "states = pd.merge(states_reps, states_abv, on='state').drop(columns=['Abbreviation','state'])\n",
    "\n",
    "# Loop through states dataframe and create list of all URLs\n",
    "for index in states.index:\n",
    "    code = states.loc[index]['Code']\n",
    "    districts = states.loc[index]['representatives']\n",
    "    \n",
    "    for dist in range(1, districts+1):\n",
    "        URL = 'https://www.opensecrets.org/races/summary?cycle=2020&id={}{}&spec=N'.format(code, str(dist).zfill(2))\n",
    "        URLs.append(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Scrape the data for all Districts for each US State\n",
    "# Initialize empty dataframe to add data to\n",
    "US = pd.DataFrame()\n",
    "\n",
    "# Loop through lists of URLs and extract data from each\n",
    "for URL in URLs:\n",
    "    response = requests.get(URL)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    soup = BS(response.text)\n",
    "    loop_df = pd.read_html(str(soup.find('table')))[0]\n",
    "    \n",
    "    # Create state and district columns\n",
    "    for index in loop_df.index:    \n",
    "        loop_df.at[index, 'state'] = re.findall(r'id=\\w\\w\\d\\d', str(soup.find('link')))[0][3:5]\n",
    "        loop_df.at[index, 'district'] = re.findall(r'id=\\w\\w\\d\\d', str(soup.find('link')))[0][5:7]\n",
    "    \n",
    "    # Combine data from webscraping into one dataframe\n",
    "    if US.empty:\n",
    "        US = loop_df\n",
    "    else:\n",
    "        US = pd.concat([US, loop_df], ignore_index = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export scraped dataframe as a csv\n",
    "US.to_csv('../data/US_scraped.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
